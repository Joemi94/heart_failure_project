





%%javascript
IPython.OutputArea.prototype._should_scroll = function(lines) {
    return false;
}


#import of necessary libaries

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns 
from pandas.plotting import scatter_matrix  
from numpy import set_printoptions
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import RFE
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.linear_model import LogisticRegression as LR
from sklearn.metrics import accuracy_score


#loading the dataset 

path = r'C:\Users\User\Documents\heart_failure_project\Health_data_proj1.csv'
df = pd.read_csv(path) 





#Peeking at the Data
df.head(20) 


#peeking at the last 20 rows

df.tail(20) 


#inspecting the shape of the dataset
df.shape 


#Exploring the Data types
df.dtypes 


#a checking for missing values
df.info() 


#b: checking for missing values
df.isna().sum() 


#checking for duplicate value
df.duplicated().sum() 


#inspecting for uniques values within features
cols = df.columns
for col in cols:
    print(f'Unique value in {col} = {df[col].unique()}')
    print(f'No. of Unique value in {col} = {df[col].nunique()} \n') 


#checking for data imbalance in the target features
print(df.stroke.value_counts()) 
sns.countplot(x='stroke', data = df) 


#inspecting for outliers 
df.plot(kind='box', subplots = True, sharex = False, layout = (4,4), figsize = (20,15))


#inspecting the statistical features of the data
df.describe().T 








#Correcting Labeling Error
df['gender'] = df['gender'].replace({'Fe5male':'Female', 'f ':'Female','MalE':'Male', 'M ':'Male', 'MALE':'Male'})
df['ever_married'] = df['ever_married'].replace(['Y','N'], ['Yes','No'])
df['work_type'] = df['work_type'].replace(['Private  ','Govt_job   ','childreen','Self-  employed'],
                                          ['Private','Govt_job','children','Self-employed'])
df['Residence_type'] = df['Residence_type'].replace(['rural l','Urb '], ['Rural','Urban'])
df['smoking_status'] = df['smoking_status'].replace(['Unk4nown','smokes   ', 'never smoked   ','never   smoked'],
                                                  ['Unknown','smokes','never smoked', 'never smoked'])


cols = df.columns
for col in cols:
    print(f'Unique value in {col} = {df[col].unique()}')
    print(f'No. of Unique value in {col} = {df[col].nunique()} \n') 


#Dropping Irrelevant features
df.drop(['id'], axis = 1, inplace = True)





#Generating a countplot of all the input variable against the output variable

cols = ['gender', 'hypertension', 'heart_disease', 'ever_married',
       'work_type', 'Residence_type', 'smoking_status', 'stroke']
for col in cols[1:]:
    g = sns.catplot(x=col, col='stroke', kind='count', data=df, sharey=False)
    g.set_xticklabels(rotation=60)





#Generating a pairwise distribution
scatter_matrix(df)
plt.gcf().set_size_inches((16,16)) 
plt.show() 





#Plotting the density plot for the continous features 
df.plot(kind='density', subplots = True, layout = (3,5), sharex = False)
plt.gcf().set_size_inches((20,20))
plt.show() 





#Treating for Duplicates values
dfCopy = df.drop_duplicates(inplace = False)  


#Treating for Missing value by dropping all the missing values
dfClean = dfCopy.dropna(inplace = False)  


continousVar = dfClean.select_dtypes(include = 'number')
categoryVar = dfClean.select_dtypes(include = 'object')


#Detecting Outliers
cols = continousVar.columns  
def outliers_con(col):
    q3, q1 = np.percentile(col, [75, 25])
    iqr = q3 - q1
    upperLimit = q3 + 1.5*iqr
    lowerLimit = q1 - 1.5*iqr
    return upperLimit, lowerLimit

#Detecting Outliers
for col in cols:
    print(f'Column : {col}')
    upper, lower = outliers_con(continousVar[col]) 
    print(f'Upper Limit = {upper}')
    print(f'Lower Limit = {lower}')
    
    total_outliers = len(continousVar.loc[continousVar[col]< lower, col]) + len(continousVar.loc[continousVar[col] > upper, col])
    percent = (total_outliers/len(continousVar.index))*100
    print(f'Percentage of Outliers = {percent}')
    print('-'*40)


#Reducing skewness in the data
continousVar[['avg_glucose_level','bmi']] = np.log1p(continousVar[['avg_glucose_level','bmi']]) 


continousVar['avg_glucose_level'].plot(kind='box')





categoryVar


#Using label Encoder to encode the categorical features
encoder = LabelEncoder()
for var in categoryVar.columns.tolist():
    categoryVar[var] = encoder.fit_transform(categoryVar[var])


categoryVar 


#creating a new Dataframe by joining the two subset
newDf = categoryVar.join(continousVar)  
newDf 


#extracting the column names into a variable
col = newDf.columns.tolist()
col = col[:-1]
col 


#inspecting Statistic Property
newDf.describe().T 


#Function to automate rescaling, Normalizing and Standardization
def normalizing(df, col, target):
    
    def rescaling(df):
        #This function perform rescaling task
        data = df.values
        X = data[:,:-1]
        Y = data[:,-1]
        
        scaler = MinMaxScaler(feature_range=(0,1))
        x = scaler.fit_transform(X)
        set_printoptions(precision=3)
        
        return x, Y
    def standardization(x):
        #This function standardie the dataset
        scaler = StandardScaler().fit(x)
        xX = scaler.transform(x)
        set_printoptions(precision=3)
        return xX
    
    x, Y = rescaling(df)
    
    xX = standardization(x)
    #The code below normalier the dataset
    scaler = Normalizer().fit(xX)
    normData = scaler.transform(xX)
    set_printoptions(precision = 3)
    
    #This code converts the dataset back to a dataframe
    normDf = pd.DataFrame(normData, columns = col)
    normDf[target] = Y
    
    return normDf
        


#Calling the Data transformation function
target = 'stroke'
normDf = normalizing(newDf, col, target)


#Inspecting the statistical properties after normalising 
normDf.describe().T 





#Testing for Multi-colinearity
plt.figure(figsize= (16,6))  #setting the canvas size
sns.heatmap(normDf.corr(), annot=True) 








#implementing KFold Validation
def autoFeaureModeler(data, numFea, Seed, modelType):
    
    def feaSelector(X, y, numFea):
        
        model = modelType() 
        rfe = RFE(model, n_features_to_select= numFea)
        fit = rfe.fit(X,y)
        
        return list(fit.ranking_) 
    
    def featureSelector(data, rankList):
        cols = data.columns
        cols = list(cols)[0:(len(cols)-1)]
        
        selCols = []

        for col, rank  in zip(cols,rankList):
            if rank ==1:
                selCols.append(col)
        
        X = data[selCols].values
        return X, selCols
    
    numCols = len(list(data.columns)) # number of columns in data
    X = data.values[:,0:(numCols-1)]
    y = data.values[:,(numCols-1)] 
    
    selectedCols = feaSelector(X, y, numFea) # list of column rankings
    X, feat = featureSelector(data, selectedCols)



    seed = Seed # ensure reproducable results from the model
    numFolds = 7
    
    kfold = KFold(n_splits=numFolds, shuffle=True, random_state=seed)

    model = modelType()
    result = cross_val_score(model, X, y, cv=kfold)
    
    modelScore =result.mean()
    
    return numFea, selectedCols, modelScore, feat 





"""This Function evaluate the performance of a given model and returns the model accuracy score,
and the features selected"""
def modelSelection(featureSearch, data,Seed, model):
    result = list()
    selectedFeatList = list()
    
    for numFea in featureSearch:
        numFea, selectedCols, modelScore,feat = autoFeaureModeler(data, numFea, Seed, model)    
        result.append(modelScore)
        selectedFeatList.append(feat)
    
    for numFea, score, feat, in zip(featureSearch, result, selectedFeatList):
        if score == max(result):
            nFeat = numFea
            accuracyScore = score
            featSelected = feat
    return nFeat, accuracyScore, featSelected


"""This code calls the model selection functions passing different models type and returning 
the accuracy of the model, number of feature selected and names of the features."""

featureSearch = [3,4,6,7,8,9,] #list of possible number of features to select
data = normDf 
Seed = 7
modelList = [RFC, DTC, LR] #list of different models 

count = 1
for model in modelList:
    nFeat, accuracyScore, featSelected = modelSelection(featureSearch, data,Seed, model) 
    modelName = ['Random Forest Classifier', 'Decision Tree Classifier', 'Logistic Regression']
    if count ==1:
        print(f'Model #{count}: {modelName[0]}')
    elif count == 2:
        print(f'Model #{count}: {modelName[1]}')
    else:
        print(f'Model #{count}: {modelName[2]}')
         
    print(f'Selected Feature = {featSelected}') 
    print(f'Best Number of Features is {nFeat} Accuracy = {accuracyScore} \n')
    count+=1
















